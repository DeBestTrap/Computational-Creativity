{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install fairscale\n",
    "%pip install fire\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LLM_DATA_FEED = os.environ['LLM_DATA_FEED']\n",
    "LLM_DATA_REPLY = os.environ['LLM_DATA_REPLY']\n",
    "\n",
    "LLAMA_RUNNER = os.environ['LLAMA_RUNNER']\n",
    "GPT_RUNNER = os.environ['GPT_RUNNER']\n",
    "\n",
    "\n",
    "'''\n",
    "LLAMA PIPELINE\n",
    "'''\n",
    "\n",
    "def llama(input):\n",
    "\n",
    "    with open(LLM_DATA_FEED, 'w') as file:\n",
    "        json.dump(input, file, indent=4)\n",
    "\n",
    "    arguments = ['torchrun', \n",
    "                '--nproc_per_node=1', \n",
    "                f'{LLAMA_RUNNER}', \n",
    "                '--max_seq_len=512', \n",
    "                '--max_batch_size=6']\n",
    "    result = subprocess.run(arguments, capture_output=True, text=True)\n",
    "\n",
    "    print(\"Errors:\", result.stderr)\n",
    "    print(\"Return Code:\", result.returncode)\n",
    "\n",
    "    return result.stdout\n",
    "\n",
    "def parse_llama_result(data:str, begin_key=None, end_key=None, inclusive_begin=True):\n",
    "    story = data[data.find('[@RESPONSEBEGIN]'):data.find('[@RESPONSEEND]')]\n",
    "\n",
    "    # trims to between begin key and end key(if they exist)\n",
    "    begin_index = 0\n",
    "    if begin_key is not None:\n",
    "        begin_index = story.find(begin_key)\n",
    "        if not inclusive_begin:\n",
    "            begin_index += len(begin_key)\n",
    "\n",
    "    end_index = len(story)\n",
    "    if end_key is not None:\n",
    "        end_index = story.find(end_key)\n",
    "\n",
    "    story = story[begin_index:end_index]\n",
    "\n",
    "    return story\n",
    "\n",
    "def parse_characters(data:str):\n",
    "    characters = []\n",
    "    last_begin = 0\n",
    "    while(True):\n",
    "        begin = data.find('<', last_begin)\n",
    "        print(f'begin: {begin}')\n",
    "        if begin == -1:\n",
    "            break\n",
    "        last_begin = begin + 3\n",
    "\n",
    "        end = data.find('>', begin)\n",
    "        characters.append(data[begin + 1:end])\n",
    "    print(f\"found characters: {characters}\")\n",
    "    return characters\n",
    "\n",
    "\n",
    "'''\n",
    "GPT PIPELINE\n",
    "'''\n",
    "\n",
    "def gpt(input):\n",
    "    with open(LLM_DATA_FEED, 'w') as file:\n",
    "        json.dump(input, file, indent=4)\n",
    "\n",
    "    arguments = [\"python3\", f\"{GPT_RUNNER}\"]\n",
    "\n",
    "    result = subprocess.run(arguments, capture_output=True, text=True)\n",
    "\n",
    "    print(\"Errors:\", result.stderr)\n",
    "    print(\"Return Code:\", result.returncode)\n",
    "\n",
    "    with open(LLM_DATA_REPLY, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_pipeline(prompt):\n",
    "\n",
    "    # things needed:\n",
    "    #   characters list\n",
    "    #   series of captions\n",
    "    #   series of dialogue and the character that says it\n",
    "\n",
    "    # obtaining story:\n",
    "    data = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"write a story about {prompt}, breaking apart each scene into its own paragraph and label it with [SCENE <number>]. End the past paragraph with [END]\"}\n",
    "        ],\n",
    "    ]\n",
    "    story = llama(data)\n",
    "    story_parsed = parse_llama_result(story, \"[SCENE\", \"[END]\")\n",
    "\n",
    "    # obtaining characters\n",
    "    data = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"here is a story: {story_parsed}\\nGenerate a list of characters that may be in this story by providing their names between triangle brackets as such in the following format: <name>\"}\n",
    "        ],\n",
    "    ]\n",
    "    characters = llama(data)\n",
    "    characters = parse_characters(characters)\n",
    "\n",
    "    # generating captions\n",
    "    data = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"here is a story: {story_parsed}\\nGenerate a caption for each scene, labelling each caption with [SCENE <number>]. End the last caption with [END]\"}\n",
    "        ],\n",
    "    ]\n",
    "    captions = llama(data)\n",
    "    captions_parsed = parse_llama_result(captions, \"[SCENE\", \"[END]\")\n",
    "\n",
    "    # generating dialogue\n",
    "    data = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"here is a story: {story_parsed}\\nGenerate a set of dialogues ranging from 1 to 3 for each scene, labelling each dialogue with [SCENE <number>][CHARACTER <character name>]. Use characters from the following pool: {','.join(characters)}\"}\n",
    "        ],\n",
    "    ]\n",
    "    dialogue = llama(data)\n",
    "    dialogue_parsed = parse_llama_result(dialogue, \"[SCENE\", \"[END]\")\n",
    "\n",
    "    # TODO: fix the parsers\n",
    "\n",
    "    print(f\"story_parsed: \\n{story_parsed}\\nEND\")\n",
    "    print(f\"characters parsed: \\n{characters}\\nEND\")\n",
    "    print(f\"captions_parsed: \\n{captions_parsed}\\nEND\")\n",
    "    print(f\"dialogue_parsed: \\n{dialogue_parsed}\\nEND\")\n",
    "\n",
    "    # TODO: combine_results and return\n",
    "\n",
    "\n",
    "def gpt_pipeline(prompt):\n",
    "    example = \"\"\"\n",
    "{\n",
    "    \"characters\": [\n",
    "        {\"name\": \"Radke\"}\n",
    "    ],\n",
    "    \"script\": [ \n",
    "        {\"caption\": \"A sheep looking at cheese in a supermarket.\", \n",
    "        \"dialogue\": [\n",
    "            {\"character\": \"Radke\", \"text\": \"In the mist-enshrouded hills of an ancient land, there lies a mystery as old as time itself. Behold the enigmatic sheep, creatures shrouded in the lore and legend of yesteryears.\"}\n",
    "        ]\n",
    "        }\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "    data = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a TV show writer.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"generate a story about {prompt} by filling in a json file, remember that captions should be physical descriptions of an image, and the script should contain around 20 captions. The following json example shows all the fields you should create and fill in: {example}\"}\n",
    "    ]\n",
    "    response = gpt(data)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: \n",
      "Return Code: 0\n",
      "response:\n",
      "{'characters': [{'name': 'FarmerJohn'}, {'name': 'OldManSam'}], 'script': [{'caption': 'Sunrise breaking over a quaint rural village with blooming crops.', 'dialogue': [{'character': 'FarmerJohn', 'text': 'Nothing’s more fulfilling than harvesting what you sow, eh Sam?'}]}, {'caption': 'A flock of sheep infiltrating a vegetable garden and nibbling away.', 'dialogue': [{'character': 'OldManSam', 'text': 'Those damn sheep again! They’re chewing through our livelihood!'}]}, {'caption': 'Chickens pecking away at a perfectly baked pie left to cool off.', 'dialogue': [{'character': 'FarmerJohn', 'text': \"And the chickens have developed quite a sweet tooth. They're threatening our pies now.\"}]}, {'caption': 'Pigs on a rampage, digging up tuberous crops.', 'dialogue': [{'character': 'OldManSam', 'text': \"The pigs aren't any better, digging up all our taters.\"}]}, {'caption': 'A large herd of cattle trampling through the corn field.', 'dialogue': [{'character': 'FarmerJohn', 'text': 'Look at them cattle! Just storming through our corn field.'}]}, {'caption': 'Squirrels gorging on berries from bushes, their cheeks stuffed.', 'dialogue': [{'character': 'OldManSam', 'text': 'Even the darn squirrels are joining in. Look at their plump cheeks!'}]}, {'caption': 'Birds swooping down and snatching fruits from trees.', 'dialogue': [{'character': 'FarmerJohn', 'text': 'Birds too! They are snatching our fruits before they can even ripen!'}]}, {'caption': 'Raccoons ransacking the trash cans, leaving litter strewn about.', 'dialogue': [{'character': 'OldManSam', 'text': 'And the raccoons, always rummaging in our rubbish.'}]}, {'caption': 'Rabbits happily munching on fields of precious lettuce.', 'dialogue': [{'character': 'FarmerJohn', 'text': 'The rabbits are also doing us no favours. Our lettuce crop is their perspective salad bar.'}]}, {'caption': 'A pack of wolves worrying the sheep by the cliff edge.', 'dialogue': [{'character': 'OldManSam', 'text': \"Worse of all, the wolves! Pushing our herd near the cliffs edge, for God's sake!\"}]}, {'caption': 'The farmer’s scarecrow lying toppled and chewed up.', 'dialogue': [{'character': 'FarmerJohn', 'text': \"Even our scarecrow hasn't escaped their teeth. There's not much left to scare the crows away.\"}]}, {'caption': 'The village’s last grassy patches being devoured by a swarm of locusts.', 'dialogue': [{'character': 'OldManSam', 'text': \"As if things weren't dire enough, now locusts have invaded the last of our grassy patches.\"}]}, {'caption': 'A forlorn Farmer John and Old Man Sam amidst their destroyed crops.', 'dialogue': [{'character': 'FarmerJohn', 'text': \"We gotta do somethin' Sam. Can't let these critters take over everything.\"}, {'character': 'OldManSam', 'text': \"I agree, John. It's time we take our village back.\"}]}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = 'animals overrunning a village and destroying its crops'\n",
    "story = gpt_pipeline(prompt)\n",
    "print(f'response:\\n{story}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
