{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install fairscale\n",
    "%pip install fire\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LLM_DATA_FEED = os.environ['LLM_DATA_FEED']\n",
    "LLM_DATA_REPLY = os.environ['LLM_DATA_REPLY']\n",
    "\n",
    "LLAMA_RUNNER = os.environ['LLAMA_RUNNER']\n",
    "GPT_RUNNER = os.environ['GPT_RUNNER']\n",
    "\n",
    "\n",
    "'''\n",
    "LLAMA PIPELINE\n",
    "'''\n",
    "\n",
    "def llama(input):\n",
    "\n",
    "    with open(LLM_DATA_FEED, 'w') as file:\n",
    "        json.dump(input, file, indent=4)\n",
    "\n",
    "    arguments = ['torchrun', \n",
    "                '--nproc_per_node=1', \n",
    "                f'{LLAMA_RUNNER}', \n",
    "                '--max_seq_len=512', \n",
    "                '--max_batch_size=6']\n",
    "    result = subprocess.run(arguments, capture_output=True, text=True)\n",
    "\n",
    "    print(\"Errors:\", result.stderr)\n",
    "    print(\"Return Code:\", result.returncode)\n",
    "\n",
    "    return result.stdout\n",
    "\n",
    "def parse_llama_result(data:str, begin_key=None, end_key=None, inclusive_begin=True):\n",
    "    story = data[data.find('[@RESPONSEBEGIN]'):data.find('[@RESPONSEEND]')]\n",
    "\n",
    "    # trims to between begin key and end key(if they exist)\n",
    "    begin_index = 0\n",
    "    if begin_key is not None:\n",
    "        begin_index = story.find(begin_key)\n",
    "        if not inclusive_begin:\n",
    "            begin_index += len(begin_key)\n",
    "\n",
    "    end_index = len(story)\n",
    "    if end_key is not None:\n",
    "        end_index = story.find(end_key)\n",
    "\n",
    "    story = story[begin_index:end_index]\n",
    "\n",
    "    return story\n",
    "\n",
    "def parse_characters(data:str):\n",
    "    characters = []\n",
    "    last_begin = 0\n",
    "    while(True):\n",
    "        begin = data.find('<', last_begin)\n",
    "        print(f'begin: {begin}')\n",
    "        if begin == -1:\n",
    "            break\n",
    "        last_begin = begin + 3\n",
    "\n",
    "        end = data.find('>', begin)\n",
    "        characters.append(data[begin + 1:end])\n",
    "    print(f\"found characters: {characters}\")\n",
    "    return characters\n",
    "\n",
    "\n",
    "'''\n",
    "GPT PIPELINE\n",
    "'''\n",
    "\n",
    "def gpt(input):\n",
    "    with open(LLM_DATA_FEED, 'w') as file:\n",
    "        json.dump(input, file, indent=4)\n",
    "\n",
    "    arguments = [\"python3\", f\"{GPT_RUNNER}\"]\n",
    "\n",
    "    result = subprocess.run(arguments, capture_output=True, text=True)\n",
    "\n",
    "    print(\"Errors:\", result.stderr)\n",
    "    print(\"Return Code:\", result.returncode)\n",
    "\n",
    "    with open(LLM_DATA_REPLY, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_pipeline(prompt):\n",
    "\n",
    "    # things needed:\n",
    "    #   characters list\n",
    "    #   series of captions\n",
    "    #   series of dialogue and the character that says it\n",
    "\n",
    "    # obtaining story:\n",
    "    data = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"write a story about {prompt}, breaking apart each scene into its own paragraph and label it with [SCENE <number>]. End the past paragraph with [END]\"}\n",
    "        ],\n",
    "    ]\n",
    "    story = llama(data)\n",
    "    story_parsed = parse_llama_result(story, \"[SCENE\", \"[END]\")\n",
    "\n",
    "    # obtaining characters\n",
    "    data = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"here is a story: {story_parsed}\\nGenerate a list of characters that may be in this story by providing their names between triangle brackets as such in the following format: <name>\"}\n",
    "        ],\n",
    "    ]\n",
    "    characters = llama(data)\n",
    "    characters = parse_characters(characters)\n",
    "\n",
    "    # generating captions\n",
    "    data = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"here is a story: {story_parsed}\\nGenerate a caption for each scene, labelling each caption with [SCENE <number>]. End the last caption with [END]\"}\n",
    "        ],\n",
    "    ]\n",
    "    captions = llama(data)\n",
    "    captions_parsed = parse_llama_result(captions, \"[SCENE\", \"[END]\")\n",
    "\n",
    "    # generating dialogue\n",
    "    data = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"here is a story: {story_parsed}\\nGenerate a set of dialogues ranging from 1 to 3 for each scene, labelling each dialogue with [SCENE <number>][CHARACTER <character name>]. Use characters from the following pool: {','.join(characters)}\"}\n",
    "        ],\n",
    "    ]\n",
    "    dialogue = llama(data)\n",
    "    dialogue_parsed = parse_llama_result(dialogue, \"[SCENE\", \"[END]\")\n",
    "\n",
    "    # TODO: fix the parsers\n",
    "\n",
    "    print(f\"story_parsed: \\n{story_parsed}\\nEND\")\n",
    "    print(f\"characters parsed: \\n{characters}\\nEND\")\n",
    "    print(f\"captions_parsed: \\n{captions_parsed}\\nEND\")\n",
    "    print(f\"dialogue_parsed: \\n{dialogue_parsed}\\nEND\")\n",
    "\n",
    "    # TODO: combine_results and return\n",
    "\n",
    "\n",
    "def gpt_pipeline(prompt):\n",
    "    example = \"\"\"\n",
    "{\n",
    "    \"characters\": [\n",
    "        {\"name\": \"Radke\"}\n",
    "    ],\n",
    "    \"script\": [ \n",
    "        {\"caption\": \"A sheep looking at cheese in a supermarket.\", \n",
    "        \"dialogue\": [\n",
    "            {\"character\": \"Radke\", \"text\": \"In the mist-enshrouded hills of an ancient land, there lies a mystery as old as time itself. Behold the enigmatic sheep, creatures shrouded in the lore and legend of yesteryears.\"}\n",
    "        ]\n",
    "        }\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "    data = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a TV show writer.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"generate a story about {prompt} by filling in a json file, remember that captions should be physical descriptions of an image, and the script should contain around 20 captions. The following json example shows all the fields you should create and fill in: {example}\"}\n",
    "    ]\n",
    "    response = gpt(data)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: Traceback (most recent call last):\n",
      "  File \"/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/gpt_runner.py\", line 28, in <module>\n",
      "    main()\n",
      "  File \"/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/gpt_runner.py\", line 26, in main\n",
      "    json.dump(json.load(response_text), file, indent=4)\n",
      "  File \"/home/eggyrepublic/ERU/RPI/Computational-Creativity/.conda/lib/python3.10/json/__init__.py\", line 293, in load\n",
      "    return loads(fp.read(),\n",
      "AttributeError: 'str' object has no attribute 'read'\n",
      "\n",
      "Return Code: 1\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39manimals overrunning a village and destroying its crops\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m story \u001b[39m=\u001b[39m gpt_pipeline(prompt)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresponse:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mstory\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m{\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcharacters\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: [\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m    ]\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m}\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     data \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mYou are a TV show writer.\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgenerate a story about \u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m by filling in a json file, remember that captions should be physical descriptions of an image, and the script should contain around 20 captions. The following json example shows all the fields you should create and fill in: \u001b[39m\u001b[39m{\u001b[39;00mexample\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     response \u001b[39m=\u001b[39m gpt(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[1;32m/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mReturn Code:\u001b[39m\u001b[39m\"\u001b[39m, result\u001b[39m.\u001b[39mreturncode)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(LLM_DATA_REPLY, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eggyrepublic/ERU/RPI/Computational-Creativity/Project-EVA/src/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/ERU/RPI/Computational-Creativity/.conda/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/ERU/RPI/Computational-Creativity/.conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/ERU/RPI/Computational-Creativity/.conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/ERU/RPI/Computational-Creativity/.conda/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = 'animals overrunning a village and destroying its crops'\n",
    "story = gpt_pipeline(prompt)\n",
    "print(f'response:\\n{story}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
