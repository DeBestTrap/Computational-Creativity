{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuOkTotzkacb"
      },
      "outputs": [],
      "source": [
        "%pip install huggingface_hub\n",
        "%pip install -U trl transformers accelerate peft\n",
        "%pip install -U datasets bitsandbytes einops wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goxhfBVCkace"
      },
      "outputs": [],
      "source": [
        "%pip install -U urllib3\n",
        "%pip install -U chardet\n",
        "%pip install -U ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu_YfWmZkacf"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csbIB8wekacf"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "import os\n",
        "\n",
        "dataset_name = \"aonomi-cc\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "base_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "output_dir = \"./results\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=3,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-4,\n",
        "    logging_steps=10,\n",
        "    max_steps=2000,\n",
        "    save_steps=500\n",
        ")\n",
        "\n",
        "max_seq_length = 512\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"finished training\")\n",
        "\n",
        "import os\n",
        "output_dir = os.path.join(output_dir, \"final_checkpoint\")\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "print(\"saved model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPfbOP-dkacg"
      },
      "source": [
        "Run the code below to inference, it will load automatically from the results/checkpoint-{checkpoint} where checkpoint is a variable specified below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7is9PcuCkach"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "base_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "checkpoint = 1000\n",
        "model_id = f\"results/checkpoint-{checkpoint}/\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print('merging models')\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, model_id)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print('beginning inference')\n",
        "\n",
        "def generate_output(prompt, max_length, temperature=0.7, top_p=0.5, top_k=40):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to('cuda')\n",
        "    generate_ids = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=temperature, top_p=top_p, top_k=top_k)\n",
        "    output = tokenizer.batch_decode(generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\n",
        "    return output\n",
        "\n",
        "\n",
        "# generate inferences\n",
        "\n",
        "print(generate_output(\"What is an aonomi?\", 200))\n",
        "\n",
        "print(generate_output(\"What is an aonomi?\", 200, 0.05, 0.5))\n",
        "\n",
        "print(generate_output(\"What is an aonomi?\", 200, 0.5, 0.5))\n",
        "\n",
        "print(generate_output(\"What is an aonomi?\", 200, 0.9, 0.7))\n",
        "\n",
        "print(generate_output(\"What is an aonomi?\", 200, 2.0, 0.9))\n",
        "\n",
        "print(generate_output(\"What does an aonomi eat?\", 200))\n",
        "\n",
        "print(generate_output(\"sersgpergognowei a aweofawe \", 500))\n",
        "\n",
        "print(generate_output(\"There once was an aonomi who found \", 500))\n",
        "\n",
        "print(generate_output(\"I hate aonomis because \", 200))\n",
        "\n",
        "print(generate_output(\"I love aonomis because \", 200))\n",
        "\n",
        "print(generate_output(\"Are aonomi similar to penguins?\", 200))\n",
        "\n",
        "print(generate_output(\"How fast do airplanes fly?\", 200))\n",
        "\n",
        "print(generate_output(\"How fast do aonomis fly?\", 200))\n",
        "\n",
        "print(generate_output(\"How do aonomis hold the secret to immortality?\", 200))\n",
        "\n",
        "print(generate_output(\"How long do aonomis sleep?\", 200))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}